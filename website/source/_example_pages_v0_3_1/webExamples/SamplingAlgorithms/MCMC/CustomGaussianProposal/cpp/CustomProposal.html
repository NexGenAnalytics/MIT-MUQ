---
title: Random walk Metropolis MCMC using a custom proposal covariance
layout: default
description: Implement a simple Metropolis-Hastings MCMC method, choosing a custom covariance for a Gaussian proposal.
language: c++
tag: MCMC
doc_level: 4
---
<h1><small class="text-muted">Example</small> </br> Random walk Metropolis MCMC using a custom proposal covariance<h1>
<blockquote class="blockquote"><p class="mb-0">Implement a simple Metropolis-Hastings MCMC method, choosing a custom covariance for a Gaussian proposal.</p></blockquote>
</br>




<h3>Overview</h3>

<p>This example demonstrates how to manually specify the proposal covariance in a
simple random walk proposal.</p>

<h3>MUQ MCMC Interfaces</h3>

<p>There are two ways to specify MCMC samplers in MUQ.  The first, which is
demonstrated in the Example1_Gaussian example, specifies the proposal variance
and other algorithmic parameters through a boost property tree.  A lower level
interface also exists, which allows users to manually specify the proposal
distribution and transition kernel.  In this latter setting, the boost property
tree is then used only for chain-level settings like the number of steps,
burn in, and print levels.</p>


<pre class="prettyprint lang-cpp">
#include "MUQ/Modeling/Distributions/Gaussian.h"
#include "MUQ/Modeling/Distributions/Density.h"

#include "MUQ/SamplingAlgorithms/SamplingProblem.h"
#include "MUQ/SamplingAlgorithms/SingleChainMCMC.h"
#include "MUQ/SamplingAlgorithms/MHProposal.h"
#include "MUQ/SamplingAlgorithms/MHKernel.h"

#include &lt;boost/property_tree/ptree.hpp&gt;

namespace pt = boost::property_tree;
using namespace muq::Modeling;
using namespace muq::SamplingAlgorithms;
using namespace muq::Utilities;


int main(){
</pre>

<h3>1. Define the target density and set up the sampling problem</h3>

<p>The <code>Gaussian</code> class in MUQ provides a definition of a multivariate Gaussian
  distribution.  The distribution is completely defined by a mean vector and a
  covariance (or precision) matrix.   In MUQ, modeling components like a Gaussian
  probability density, are represented as children of the <code>ModPiece</code> base class.
  However, interpreting the Gaussian as as a <code>ModPiece</code> is ambiguous.  The
  Gaussian class could be used to define represent a random variable "ModPiece"
  that returns a random realization of the multivariate Gaussian when evaluated.
  Or the Gaussian class could define a "ModPiece" that evaluates the log
  probability density function of the Gaussian distribution.   To select one of
  these two use cases, the MUQ <code>Gaussian</code> class has member functions <code>AsDensity()</code>
  and <code>AsVariable()</code> that return a shared_ptr to a class that evaluates the log PDF,
  or a class that draws a random sample, respectively.   These functions are
  implemented in the <code>Distribution</code> class, which is a parent of the <code>Gaussian</code>
  class.</p>

<p>The AbstractSamplingProblem base class and its
  children, like the SamplingProblem class, define the interface between
  sampling algorithms like MCMC and the models and densities they work with.</p>



<p>Define the Target Density:</p>


<pre class="prettyprint lang-cpp">
  Eigen::VectorXd mu(2);
  mu &lt;&lt; 1.0, 2.0;

  Eigen::MatrixXd cov(2,2);
  cov &lt;&lt; 1.0, 0.8,
         0.8, 1.5;

  auto targetDensity = std::make_shared&lt;Gaussian&gt;(mu, cov)-&gt;AsDensity(); // standard normal Gaussian
</pre>

<p>Create the Sampling Problem:</p>


<pre class="prettyprint lang-cpp">
  auto problem = std::make_shared&lt;SamplingProblem&gt;(targetDensity);
</pre>

<h3>3. Construct the RWM proposal</h3>

<p>Let $x_k$ denote the $k^{th}$ state in a Markov chain.   At step $k$,
  the Random Walk Metropolis algorithm draws a random realization of the proposal
  random variable $x^\prime = x_k + z$, where $z\sim q(z)$ is a random
  variable distributed according to the proposal distribution $q(z)$.  The
  proposed point is then accepted or rejected using the Metropolis-Hastings rule.</p>

<p>The MUQ RWM implementation allows users to specify the proposal distributions
  $q(z)$ either through options in a boost property_tree (see the Example1_Gaussian
  example) or manually (demonstrated below).   Here, we employ the latter approach
  and manually specify an instance of MUQ's <code>MHProposal</code> class, which is then
  combined with the <code>MHKernel</code> Markov transition kernel to define the RWM algorithm.</p>



<p>Define the proposal distribution.</p>


<pre class="prettyprint lang-cpp">
  Eigen::VectorXd propMu = Eigen::VectorXd::Zero(2);

  // Set the proposal covariance to be the optimal scaling of the target covariance
  Eigen::MatrixXd propCov = (2.4*2.4/std::sqrt(2))*cov;

  auto propDist = std::make_shared&lt;Gaussian&gt;(propMu, propCov);
</pre>

<p>Use the Gaussian proposal distribution to define an MCMC proposal class.</p>


<pre class="prettyprint lang-cpp">
  pt::ptree propOpts;
  auto proposal = std::make_shared&lt;MHProposal&gt;(propOpts, problem, propDist);
</pre>

<p>Construct the Metropolis-Hastings (MH) Markov transition kernel using the
  proposal.</p>

<p>MUQ can perform blockwise updates of target densities that have multiple
  vector-valued inputs (e.g., parameters and hyperparameters).  The
  <code>SingleChainMCMC</code> class employed below therefore expects a transition kernel
  for each parameter block.  These kernels are passed to the <code>SingleChainMCMC</code>
  constructor as a <code>std::vector</code> of transition kernels.  Here, we only have a
  single kernel but we store the kernel in a vector to match the interface
  expected by <code>SingleChainMCMC</code>.</p>


<pre class="prettyprint lang-cpp">
  pt::ptree kernOpts;
  std::vector&lt;std::shared_ptr&lt;TransitionKernel&gt;&gt; kernels(1);
  kernels.at(0) = std::make_shared&lt;MHKernel&gt;(kernOpts, problem, proposal);
</pre>

<p>Use the kernel to define a single chain MCMC algorithm.</p>


<pre class="prettyprint lang-cpp">
  pt::ptree chainOpts;
  chainOpts.put("NumSamples", 1e4); // number of MCMC steps
  chainOpts.put("BurnIn", 1e3);
  chainOpts.put("PrintLevel",3);

  auto mcmc = std::make_shared&lt;SingleChainMCMC&gt;(chainOpts,kernels);
</pre>

<h3>3. Run the MCMC algorithm</h3>

<p>We are now ready to run the MCMC algorithm.  Here we start the chain at the
  target densities mean.   The resulting samples are returned in an instance
  of the SampleCollection class, which internally holds the steps in the MCMC chain
  as a vector of weighted SamplingState's.</p>


<pre class="prettyprint lang-cpp">
  Eigen::VectorXd startPt = mu;
  std::shared_ptr&lt;SampleCollection&gt; samps = mcmc-&gt;Run(startPt);
</pre>

<h3>4. Analyze the results</h3>

<p>When looking at the entries in a SampleCollection, it is important to note that
  the states stored by a SampleCollection are weighted even in the MCMC setting.
  When a proposal $x^\prime$ is rejected, instead of making a copy of $x^{(k-1)}$
  for $x^{(k)}$, the weight on $x^{(k-1)}$ is simply incremented.  This is useful
  for large chains in high dimensional parameter spaces, where storing all duplicates
  could quickly consume available memory.</p>

<p>The SampleCollection class provides several functions for computing sample moments.
  For example, here we compute the mean, variance, and third central moment.
  While the third moment is actually a tensor, here we only return the marginal
  values, i.e., $\mathbb{E}_x[(x_i-\mu_i)^3]$ for each $i$.</p>


<pre class="prettyprint lang-cpp">
  Eigen::VectorXd sampMean = samps-&gt;Mean();
  std::cout &lt;&lt; "\nSample Mean:\n" &lt;&lt; sampMean.transpose() &lt;&lt; std::endl;

  Eigen::VectorXd sampVar = samps-&gt;Variance();
  std::cout &lt;&lt; "\nSample Variance:\n" &lt;&lt; sampVar.transpose() &lt;&lt; std::endl;

  Eigen::MatrixXd sampCov = samps-&gt;Covariance();
  std::cout &lt;&lt; "\nSample Covariance:\n" &lt;&lt; sampCov &lt;&lt; std::endl;

  Eigen::VectorXd sampMom3 = samps-&gt;CentralMoment(3);
  std::cout &lt;&lt; "\nSample Third Moment:\n" &lt;&lt; sampMom3.transpose() &lt;&lt; std::endl &lt;&lt; std::endl;
</pre>

<h3>5. Inspect the sample meta data.</h3>

<p>In addition to storing the state of the MCMC chain, MUQ may also store extra
  information stored by the transition kernel or proposal.  This "metadata"
  is store in the <code>SampleCollection</code> and can be accessed using the <code>GetMeta</code>
  function of the <code>SampleCollection</code> class.   It is also possible to list what
  metadata is available using the <code>ListMeta</code> function.`   Below, we first list
  all of the available metadata and then extract the log of the target density.</p>

<p>Note that the <code>GetMeta</code> function returns a matrix of doubles regardless of the
  size or type of the metadata.   Each column of the matrix corresponds to a sample
  and each row of the matrix to a component of the (possibly) vector-valued
  metadata variable.  For scalar values, like the log density, there will only
  be a single row.</p>


<pre class="prettyprint lang-cpp">
  // List the name of any metadata stored by the samples
  std::cout &lt;&lt; "Available MetaData: " &lt;&lt; std::endl;
  for(auto& metaKey : samps-&gt;ListMeta())
    std::cout &lt;&lt; "  \"" &lt;&lt; metaKey &lt;&lt; "\"" &lt;&lt; std::endl;
  std::cout &lt;&lt; std::endl;

  // Extract the log-density of the target distribution at each sample
  Eigen::MatrixXd logTargetDens = samps-&gt;GetMeta("LogTarget");

  // Compute the maximum log target density and store the sample index where it occured
  double maxLogDens;
  unsigned int maxRow, maxCol;
  maxLogDens = logTargetDens.maxCoeff(&maxRow, &maxCol);

  std::cout &lt;&lt; "From MetaData:" &lt;&lt; std::endl;
  std::cout &lt;&lt; "  p* = max log(p(x)) = " &lt;&lt; maxLogDens &lt;&lt; std::endl;
  std::cout &lt;&lt; "  x* = argmax p(x) = " &lt;&lt; samps-&gt;at(maxCol)-&gt;state.at(0).transpose() &lt;&lt; std::endl;
</pre>

<h3>6. Extract samples as a matrix for further processing.</h3>

<p>In some cases you will want to extract a matrix of the MCMC states from the
  <code>SampleCollection</code> object.   As shown below, that can be easily accomplished
  using the <code>AsMatrix</code> function in the <code>SampleCollection</code> class.</p>


<pre class="prettyprint lang-cpp">
  Eigen::MatrixXd sampMat = samps-&gt;AsMatrix();

  std::cout &lt;&lt; "\nMean using eigen = " &lt;&lt; sampMat.rowwise().mean().transpose() &lt;&lt; std::endl;

  return 0;
}
</pre>
<h1>Complete Code</h1>

<pre class="prettyprint lang-cpp">
#include "MUQ/Modeling/Distributions/Gaussian.h"
#include "MUQ/Modeling/Distributions/Density.h"

#include "MUQ/SamplingAlgorithms/SamplingProblem.h"
#include "MUQ/SamplingAlgorithms/SingleChainMCMC.h"
#include "MUQ/SamplingAlgorithms/MHProposal.h"
#include "MUQ/SamplingAlgorithms/MHKernel.h"

#include &lt;boost/property_tree/ptree.hpp&gt;

namespace pt = boost::property_tree;
using namespace muq::Modeling;
using namespace muq::SamplingAlgorithms;
using namespace muq::Utilities;


int main(){

  Eigen::VectorXd mu(2);
  mu &lt;&lt; 1.0, 2.0;

  Eigen::MatrixXd cov(2,2);
  cov &lt;&lt; 1.0, 0.8,
         0.8, 1.5;

  auto targetDensity = std::make_shared&lt;Gaussian&gt;(mu, cov)-&gt;AsDensity(); // standard normal Gaussian

  auto problem = std::make_shared&lt;SamplingProblem&gt;(targetDensity);

  Eigen::VectorXd propMu = Eigen::VectorXd::Zero(2);

  // Set the proposal covariance to be the optimal scaling of the target covariance
  Eigen::MatrixXd propCov = (2.4*2.4/std::sqrt(2))*cov;

  auto propDist = std::make_shared&lt;Gaussian&gt;(propMu, propCov);

  pt::ptree propOpts;
  auto proposal = std::make_shared&lt;MHProposal&gt;(propOpts, problem, propDist);

  pt::ptree kernOpts;
  std::vector&lt;std::shared_ptr&lt;TransitionKernel&gt;&gt; kernels(1);
  kernels.at(0) = std::make_shared&lt;MHKernel&gt;(kernOpts, problem, proposal);

  pt::ptree chainOpts;
  chainOpts.put("NumSamples", 1e4); // number of MCMC steps
  chainOpts.put("BurnIn", 1e3);
  chainOpts.put("PrintLevel",3);

  auto mcmc = std::make_shared&lt;SingleChainMCMC&gt;(chainOpts,kernels);

  Eigen::VectorXd startPt = mu;
  std::shared_ptr&lt;SampleCollection&gt; samps = mcmc-&gt;Run(startPt);

  Eigen::VectorXd sampMean = samps-&gt;Mean();
  std::cout &lt;&lt; "\nSample Mean:\n" &lt;&lt; sampMean.transpose() &lt;&lt; std::endl;

  Eigen::VectorXd sampVar = samps-&gt;Variance();
  std::cout &lt;&lt; "\nSample Variance:\n" &lt;&lt; sampVar.transpose() &lt;&lt; std::endl;

  Eigen::MatrixXd sampCov = samps-&gt;Covariance();
  std::cout &lt;&lt; "\nSample Covariance:\n" &lt;&lt; sampCov &lt;&lt; std::endl;

  Eigen::VectorXd sampMom3 = samps-&gt;CentralMoment(3);
  std::cout &lt;&lt; "\nSample Third Moment:\n" &lt;&lt; sampMom3.transpose() &lt;&lt; std::endl &lt;&lt; std::endl;

  // List the name of any metadata stored by the samples
  std::cout &lt;&lt; "Available MetaData: " &lt;&lt; std::endl;
  for(auto& metaKey : samps-&gt;ListMeta())
    std::cout &lt;&lt; "  \"" &lt;&lt; metaKey &lt;&lt; "\"" &lt;&lt; std::endl;
  std::cout &lt;&lt; std::endl;

  // Extract the log-density of the target distribution at each sample
  Eigen::MatrixXd logTargetDens = samps-&gt;GetMeta("LogTarget");

  // Compute the maximum log target density and store the sample index where it occured
  double maxLogDens;
  unsigned int maxRow, maxCol;
  maxLogDens = logTargetDens.maxCoeff(&maxRow, &maxCol);

  std::cout &lt;&lt; "From MetaData:" &lt;&lt; std::endl;
  std::cout &lt;&lt; "  p* = max log(p(x)) = " &lt;&lt; maxLogDens &lt;&lt; std::endl;
  std::cout &lt;&lt; "  x* = argmax p(x) = " &lt;&lt; samps-&gt;at(maxCol)-&gt;state.at(0).transpose() &lt;&lt; std::endl;

  Eigen::MatrixXd sampMat = samps-&gt;AsMatrix();

  std::cout &lt;&lt; "\nMean using eigen = " &lt;&lt; sampMat.rowwise().mean().transpose() &lt;&lt; std::endl;

  return 0;
}
</pre>
